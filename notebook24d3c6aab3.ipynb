{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/ankur12-1610/private-ml-for-health.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:26:03.688154Z","iopub.execute_input":"2025-04-17T07:26:03.688342Z","iopub.status.idle":"2025-04-17T07:26:06.545566Z","shell.execute_reply.started":"2025-04-17T07:26:03.688326Z","shell.execute_reply":"2025-04-17T07:26:06.544837Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'private-ml-for-health'...\nremote: Enumerating objects: 458, done.\u001b[K\nremote: Counting objects: 100% (119/119), done.\u001b[K\nremote: Compressing objects: 100% (110/110), done.\u001b[K\nremote: Total 458 (delta 65), reused 18 (delta 9), pack-reused 339 (from 1)\u001b[K\nReceiving objects: 100% (458/458), 67.97 MiB | 37.02 MiB/s, done.\nResolving deltas: 100% (242/242), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd private-ml-for-health/private_training","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:26:06.546883Z","iopub.execute_input":"2025-04-17T07:26:06.547110Z","iopub.status.idle":"2025-04-17T07:26:06.552559Z","shell.execute_reply.started":"2025-04-17T07:26:06.547088Z","shell.execute_reply":"2025-04-17T07:26:06.551976Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/private-ml-for-health/private_training\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install torchcsprng==0.1.2+cu101 torch==1.7.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n!pip install opacus==0.15.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:28:20.122669Z","iopub.execute_input":"2025-04-17T07:28:20.122976Z","iopub.status.idle":"2025-04-17T07:29:51.591056Z","shell.execute_reply.started":"2025-04-17T07:28:20.122952Z","shell.execute_reply":"2025-04-17T07:29:51.590051Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://download.pytorch.org/whl/torch_stable.html\n\u001b[31mERROR: Could not find a version that satisfies the requirement torchcsprng==0.1.2+cu101 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for torchcsprng==0.1.2+cu101\u001b[0m\u001b[31m\n\u001b[0mCollecting opacus==0.15.0\n  Downloading opacus-0.15.0-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.11/dist-packages (from opacus==0.15.0) (1.26.4)\nRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.11/dist-packages (from opacus==0.15.0) (2.5.1+cu124)\nRequirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.11/dist-packages (from opacus==0.15.0) (1.15.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15->opacus==0.15.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15->opacus==0.15.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15->opacus==0.15.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15->opacus==0.15.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15->opacus==0.15.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.15->opacus==0.15.0) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3->opacus==0.15.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3->opacus==0.15.0)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3->opacus==0.15.0)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3->opacus==0.15.0)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3->opacus==0.15.0)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3->opacus==0.15.0)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3->opacus==0.15.0)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3->opacus==0.15.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3->opacus==0.15.0) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3->opacus==0.15.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.15->opacus==0.15.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.15->opacus==0.15.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.15->opacus==0.15.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.15->opacus==0.15.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.15->opacus==0.15.0) (2024.2.0)\nDownloading opacus-0.15.0-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, opacus\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 opacus-0.15.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!python3 src/dr_dataset_to_numpy.py --dataset=dr --dr_from_np=0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:30:00.588302Z","iopub.execute_input":"2025-04-17T07:30:00.588564Z","iopub.status.idle":"2025-04-17T07:41:39.914815Z","shell.execute_reply.started":"2025-04-17T07:30:00.588544Z","shell.execute_reply":"2025-04-17T07:41:39.913906Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1G-4UhPKiQY3NxQtZiWuOkdocDTW6Bw0u\nFrom (redirected): https://drive.google.com/uc?id=1G-4UhPKiQY3NxQtZiWuOkdocDTW6Bw0u&confirm=t&uuid=61950a4a-25da-418d-a3ce-ad84a4505c8d\nTo: /kaggle/working/private-ml-for-health/data/diabetic_retinopathy/images.zip\n100%|██████████████████████████████████████| 8.60G/8.60G [01:29<00:00, 96.2MB/s]\nExtracting...!\nExtracted!\n0\ntrain_dataset size: 2931\ntest_dataset size: 731\ntrain\n   2930 2915\ntest\n   730\nDone!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!python3 src/federated_main_s2.py --epochs=1 --num_users=1 --frac=1. --local_ep=100 --local_bs=50 --virtual_batch_size=50 --optimizer='sgd' --lr=0.001 --momentum=0.9 --dataset='dr' --dr_from_np=1 --gpu=\"cuda:0\" --withDP=0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:43:06.921504Z","iopub.execute_input":"2025-04-17T07:43:06.922107Z"}},"outputs":[{"name":"stdout","text":"0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 16, 16]           9,408\n       BatchNorm2d-2           [-1, 64, 16, 16]             128\n              ReLU-3           [-1, 64, 16, 16]               0\n         MaxPool2d-4             [-1, 64, 8, 8]               0\n            Conv2d-5             [-1, 64, 8, 8]           4,096\n       BatchNorm2d-6             [-1, 64, 8, 8]             128\n              ReLU-7             [-1, 64, 8, 8]               0\n            Conv2d-8             [-1, 64, 8, 8]          36,864\n       BatchNorm2d-9             [-1, 64, 8, 8]             128\n             ReLU-10             [-1, 64, 8, 8]               0\n           Conv2d-11            [-1, 256, 8, 8]          16,384\n      BatchNorm2d-12            [-1, 256, 8, 8]             512\n           Conv2d-13            [-1, 256, 8, 8]          16,384\n      BatchNorm2d-14            [-1, 256, 8, 8]             512\n             ReLU-15            [-1, 256, 8, 8]               0\n       Bottleneck-16            [-1, 256, 8, 8]               0\n           Conv2d-17             [-1, 64, 8, 8]          16,384\n      BatchNorm2d-18             [-1, 64, 8, 8]             128\n             ReLU-19             [-1, 64, 8, 8]               0\n           Conv2d-20             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-21             [-1, 64, 8, 8]             128\n             ReLU-22             [-1, 64, 8, 8]               0\n           Conv2d-23            [-1, 256, 8, 8]          16,384\n      BatchNorm2d-24            [-1, 256, 8, 8]             512\n             ReLU-25            [-1, 256, 8, 8]               0\n       Bottleneck-26            [-1, 256, 8, 8]               0\n           Conv2d-27             [-1, 64, 8, 8]          16,384\n      BatchNorm2d-28             [-1, 64, 8, 8]             128\n             ReLU-29             [-1, 64, 8, 8]               0\n           Conv2d-30             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-31             [-1, 64, 8, 8]             128\n             ReLU-32             [-1, 64, 8, 8]               0\n           Conv2d-33            [-1, 256, 8, 8]          16,384\n      BatchNorm2d-34            [-1, 256, 8, 8]             512\n             ReLU-35            [-1, 256, 8, 8]               0\n       Bottleneck-36            [-1, 256, 8, 8]               0\n           Conv2d-37            [-1, 128, 8, 8]          32,768\n      BatchNorm2d-38            [-1, 128, 8, 8]             256\n             ReLU-39            [-1, 128, 8, 8]               0\n           Conv2d-40            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-41            [-1, 128, 4, 4]             256\n             ReLU-42            [-1, 128, 4, 4]               0\n           Conv2d-43            [-1, 512, 4, 4]          65,536\n      BatchNorm2d-44            [-1, 512, 4, 4]           1,024\n           Conv2d-45            [-1, 512, 4, 4]         131,072\n      BatchNorm2d-46            [-1, 512, 4, 4]           1,024\n             ReLU-47            [-1, 512, 4, 4]               0\n       Bottleneck-48            [-1, 512, 4, 4]               0\n           Conv2d-49            [-1, 128, 4, 4]          65,536\n      BatchNorm2d-50            [-1, 128, 4, 4]             256\n             ReLU-51            [-1, 128, 4, 4]               0\n           Conv2d-52            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-53            [-1, 128, 4, 4]             256\n             ReLU-54            [-1, 128, 4, 4]               0\n           Conv2d-55            [-1, 512, 4, 4]          65,536\n      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n             ReLU-57            [-1, 512, 4, 4]               0\n       Bottleneck-58            [-1, 512, 4, 4]               0\n           Conv2d-59            [-1, 128, 4, 4]          65,536\n      BatchNorm2d-60            [-1, 128, 4, 4]             256\n             ReLU-61            [-1, 128, 4, 4]               0\n           Conv2d-62            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-63            [-1, 128, 4, 4]             256\n             ReLU-64            [-1, 128, 4, 4]               0\n           Conv2d-65            [-1, 512, 4, 4]          65,536\n      BatchNorm2d-66            [-1, 512, 4, 4]           1,024\n             ReLU-67            [-1, 512, 4, 4]               0\n       Bottleneck-68            [-1, 512, 4, 4]               0\n           Conv2d-69            [-1, 128, 4, 4]          65,536\n      BatchNorm2d-70            [-1, 128, 4, 4]             256\n             ReLU-71            [-1, 128, 4, 4]               0\n           Conv2d-72            [-1, 128, 4, 4]         147,456\n      BatchNorm2d-73            [-1, 128, 4, 4]             256\n             ReLU-74            [-1, 128, 4, 4]               0\n           Conv2d-75            [-1, 512, 4, 4]          65,536\n      BatchNorm2d-76            [-1, 512, 4, 4]           1,024\n             ReLU-77            [-1, 512, 4, 4]               0\n       Bottleneck-78            [-1, 512, 4, 4]               0\n           Conv2d-79            [-1, 256, 4, 4]         131,072\n      BatchNorm2d-80            [-1, 256, 4, 4]             512\n             ReLU-81            [-1, 256, 4, 4]               0\n           Conv2d-82            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-83            [-1, 256, 2, 2]             512\n             ReLU-84            [-1, 256, 2, 2]               0\n           Conv2d-85           [-1, 1024, 2, 2]         262,144\n      BatchNorm2d-86           [-1, 1024, 2, 2]           2,048\n           Conv2d-87           [-1, 1024, 2, 2]         524,288\n      BatchNorm2d-88           [-1, 1024, 2, 2]           2,048\n             ReLU-89           [-1, 1024, 2, 2]               0\n       Bottleneck-90           [-1, 1024, 2, 2]               0\n           Conv2d-91            [-1, 256, 2, 2]         262,144\n      BatchNorm2d-92            [-1, 256, 2, 2]             512\n             ReLU-93            [-1, 256, 2, 2]               0\n           Conv2d-94            [-1, 256, 2, 2]         589,824\n      BatchNorm2d-95            [-1, 256, 2, 2]             512\n             ReLU-96            [-1, 256, 2, 2]               0\n           Conv2d-97           [-1, 1024, 2, 2]         262,144\n      BatchNorm2d-98           [-1, 1024, 2, 2]           2,048\n             ReLU-99           [-1, 1024, 2, 2]               0\n      Bottleneck-100           [-1, 1024, 2, 2]               0\n          Conv2d-101            [-1, 256, 2, 2]         262,144\n     BatchNorm2d-102            [-1, 256, 2, 2]             512\n            ReLU-103            [-1, 256, 2, 2]               0\n          Conv2d-104            [-1, 256, 2, 2]         589,824\n     BatchNorm2d-105            [-1, 256, 2, 2]             512\n            ReLU-106            [-1, 256, 2, 2]               0\n          Conv2d-107           [-1, 1024, 2, 2]         262,144\n     BatchNorm2d-108           [-1, 1024, 2, 2]           2,048\n            ReLU-109           [-1, 1024, 2, 2]               0\n      Bottleneck-110           [-1, 1024, 2, 2]               0\n          Conv2d-111            [-1, 256, 2, 2]         262,144\n     BatchNorm2d-112            [-1, 256, 2, 2]             512\n            ReLU-113            [-1, 256, 2, 2]               0\n          Conv2d-114            [-1, 256, 2, 2]         589,824\n     BatchNorm2d-115            [-1, 256, 2, 2]             512\n            ReLU-116            [-1, 256, 2, 2]               0\n          Conv2d-117           [-1, 1024, 2, 2]         262,144\n     BatchNorm2d-118           [-1, 1024, 2, 2]           2,048\n            ReLU-119           [-1, 1024, 2, 2]               0\n      Bottleneck-120           [-1, 1024, 2, 2]               0\n          Conv2d-121            [-1, 256, 2, 2]         262,144\n     BatchNorm2d-122            [-1, 256, 2, 2]             512\n            ReLU-123            [-1, 256, 2, 2]               0\n          Conv2d-124            [-1, 256, 2, 2]         589,824\n     BatchNorm2d-125            [-1, 256, 2, 2]             512\n            ReLU-126            [-1, 256, 2, 2]               0\n          Conv2d-127           [-1, 1024, 2, 2]         262,144\n     BatchNorm2d-128           [-1, 1024, 2, 2]           2,048\n            ReLU-129           [-1, 1024, 2, 2]               0\n      Bottleneck-130           [-1, 1024, 2, 2]               0\n          Conv2d-131            [-1, 256, 2, 2]         262,144\n     BatchNorm2d-132            [-1, 256, 2, 2]             512\n            ReLU-133            [-1, 256, 2, 2]               0\n          Conv2d-134            [-1, 256, 2, 2]         589,824\n     BatchNorm2d-135            [-1, 256, 2, 2]             512\n            ReLU-136            [-1, 256, 2, 2]               0\n          Conv2d-137           [-1, 1024, 2, 2]         262,144\n     BatchNorm2d-138           [-1, 1024, 2, 2]           2,048\n            ReLU-139           [-1, 1024, 2, 2]               0\n      Bottleneck-140           [-1, 1024, 2, 2]               0\n          Conv2d-141            [-1, 512, 2, 2]         524,288\n     BatchNorm2d-142            [-1, 512, 2, 2]           1,024\n            ReLU-143            [-1, 512, 2, 2]               0\n          Conv2d-144            [-1, 512, 1, 1]       2,359,296\n     BatchNorm2d-145            [-1, 512, 1, 1]           1,024\n            ReLU-146            [-1, 512, 1, 1]               0\n          Conv2d-147           [-1, 2048, 1, 1]       1,048,576\n     BatchNorm2d-148           [-1, 2048, 1, 1]           4,096\n          Conv2d-149           [-1, 2048, 1, 1]       2,097,152\n     BatchNorm2d-150           [-1, 2048, 1, 1]           4,096\n            ReLU-151           [-1, 2048, 1, 1]               0\n      Bottleneck-152           [-1, 2048, 1, 1]               0\n          Conv2d-153            [-1, 512, 1, 1]       1,048,576\n     BatchNorm2d-154            [-1, 512, 1, 1]           1,024\n            ReLU-155            [-1, 512, 1, 1]               0\n          Conv2d-156            [-1, 512, 1, 1]       2,359,296\n     BatchNorm2d-157            [-1, 512, 1, 1]           1,024\n            ReLU-158            [-1, 512, 1, 1]               0\n          Conv2d-159           [-1, 2048, 1, 1]       1,048,576\n     BatchNorm2d-160           [-1, 2048, 1, 1]           4,096\n            ReLU-161           [-1, 2048, 1, 1]               0\n      Bottleneck-162           [-1, 2048, 1, 1]               0\n          Conv2d-163            [-1, 512, 1, 1]       1,048,576\n     BatchNorm2d-164            [-1, 512, 1, 1]           1,024\n            ReLU-165            [-1, 512, 1, 1]               0\n          Conv2d-166            [-1, 512, 1, 1]       2,359,296\n     BatchNorm2d-167            [-1, 512, 1, 1]           1,024\n            ReLU-168            [-1, 512, 1, 1]               0\n          Conv2d-169           [-1, 2048, 1, 1]       1,048,576\n     BatchNorm2d-170           [-1, 2048, 1, 1]           4,096\n            ReLU-171           [-1, 2048, 1, 1]               0\n      Bottleneck-172           [-1, 2048, 1, 1]               0\nAdaptiveAvgPool2d-173           [-1, 2048, 1, 1]               0\n          Linear-174                  [-1, 100]         204,900\n================================================================\nTotal params: 23,712,932\nTrainable params: 23,712,932\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 5.86\nParams size (MB): 90.46\nEstimated Total Size (MB): 96.33\n----------------------------------------------------------------\n/kaggle/working/private-ml-for-health/private_training/src/update_s2.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return image.clone().detach(), torch.tensor(label)\n\nEpoch: 1\nAverage train loss: 1.3910168090770985\nTest Accuracy: 65.53%\n\nEpoch: 2\nAverage train loss: 1.1992909543473145\nTest Accuracy: 66.89%\n\nEpoch: 3\nAverage train loss: 1.1145927649119804\nTest Accuracy: 66.62%\n\nEpoch: 4\nAverage train loss: 1.069767684001347\nTest Accuracy: 68.81%\n\nEpoch: 5\nAverage train loss: 1.0372636455914068\nTest Accuracy: 69.77%\n\nEpoch: 6\nAverage train loss: 1.0103790614454224\nTest Accuracy: 67.31%\n\nEpoch: 7\nAverage train loss: 0.9869594086567168\nTest Accuracy: 67.72%\n\nEpoch: 8\nAverage train loss: 0.971932462577162\nTest Accuracy: 70.31%\n\nEpoch: 9\nAverage train loss: 0.9644817803554607\nTest Accuracy: 69.08%\n\nEpoch: 10\nAverage train loss: 0.9534716850724714\nTest Accuracy: 71.00%\n","output_type":"stream"}],"execution_count":null}]}